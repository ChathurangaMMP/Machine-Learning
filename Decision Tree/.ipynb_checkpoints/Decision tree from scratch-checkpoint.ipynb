{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree algorithm from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **classification algorithm**, and it is used to represent all the possible solutions to a decision in a graphical way. it is much similar to the way what we used to make decisions using tree methods manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure of building this algorithm from base is little bit long. For the implementation in this notebook, a very simple dataset has used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=[\n",
    "    ['Green',3,'Mango'],\n",
    "    ['Yellow',3,'Mango'],\n",
    "    ['Red',1,'Grape'],\n",
    "    ['Red',1,'Grape'],\n",
    "    ['Yellow',3,'Lemon']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can give headers for data columns. They will be useful for handling our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header=['color','diameter','label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement this algorithm, lots of functions and classes are required. Let’s define them one by one. \n",
    "\n",
    "First, we have to define a function to get the **unique values** of a column. There are two parameters in **unique_vals** function. \n",
    "**rows** parameter represents a list of lists, and each list includes the values corresponding to a row. \n",
    "**col** is an integer parameter and it holds the index of the column which we want to get unique values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals(rows,col):\n",
    "    return set([row[col] for row in rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the count of each label has got. **class_count** is a single parameter function, and the **rows** parameter represents a list of lists which have row data. The label name and the count of it are stored in the **counts** dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_count(rows):\n",
    "    counts={}\n",
    "    for row in rows: \n",
    "        label=row[-1] # in this dataset the label is always last column\n",
    "        if label not in counts:\n",
    "            counts[label]=0\n",
    "        counts[label]+=1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need a function to check whether a value is **numeric** or not. For that the **is_numeric** function has developed. The parameter **value** is the value which we want to check the data type. The method used in this function is the **isinstance(value, data type)**. It simply returns **True** or **False** by comparing the *value* and *data type* parameters. This is a built-in python method that can be used to verify the data type of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(value):\n",
    "    return isinstance(value,int) or isinstance(value,float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Tree builds, the **partitioning** is an important stage. It divides the dataset. To do so, we need **questions**. The following **class** is coded for that. \n",
    "\n",
    "**__init__** method defines two variables called **column** and **value**. Those are the **column index** and the **feature value** respectively. Actual dataset values are going to be compared with these.\n",
    "\n",
    "Next the **match** method compares the above two values and the corresponding actual dataset values to check whether they satisfy our conditions in the example. The two conditions that are used here, \n",
    "- If the value is *numeric* then it should be greater than the *value* defined in the class.\n",
    "- If the value is *not numeric* then it should be equal to the *label* in the *column* index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question:\n",
    "    \n",
    "    def __init__(self,column,value):\n",
    "        self.column=column\n",
    "        self.value=value\n",
    "        \n",
    "    def match(self,example):\n",
    "        # compare the feature value in an example with the feature value in this question\n",
    "        val=example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val>=self.value\n",
    "        else:\n",
    "            return val==self.value\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # just a helper to represent question in readable format\n",
    "        condition='=='\n",
    "        if is_numeric(self.value):\n",
    "            condition='>='\n",
    "        return 'Is %s %s %s?'%(header[self.column],condition,str(self.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can divide the dataset into partitions by matching the dataset rows with our question. If a row matches with the question, then it is added to the **true rows** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(rows,question):\n",
    "    \n",
    "    true_rows,false_rows=[],[]\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "            \n",
    "    return true_rows,false_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our goal is finding the best split for the dataset. To do that we can use two important parameters. They are **Gini Impurity** and **Information Gain**. \n",
    "\n",
    "Gini impurity is a measure of correctness. It represents *the probability of classifying a row incorrectly*. To split the node, we choose the row which has **the lowest** Gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main definitions for calculating Gini impurity.\n",
    "\n",
    "- Let’s take a dataset *T* which has samples from *k* classes. The probability of sample belonging to class *i* is *Pi*. Then the Gini impurity of the dataset *T* can be denoted as,\n",
    "\n",
    "\\begin{equation}\n",
    "Gini(T) = 1 - \\sum\\limits_{i=1}^{k} {(P_i)^2}\n",
    "\\end{equation}\n",
    "\n",
    "- The following way is also correct. \n",
    "\n",
    "\\begin{equation}\n",
    "Gini(T) = \\sum\\limits_{i=1}^{k} {P_i * (1 - P_i)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(rows):\n",
    "    \n",
    "    counts=class_count(rows)\n",
    "    impurity=1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl=counts[lbl]/float(len(rows))\n",
    "        impurity-=prob_of_lbl**2\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**information gain** is used to determine the homogeneous level of a branch. When we find the best split, it is important to take the branches which have the highest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(left,right,current_uncertainty):\n",
    "   \n",
    "    # the uncertainty of the starting node, minus the weight impurity of two child nodes\n",
    "    p=float(len(left)/len(right)+len(right))\n",
    "    return current_uncertainty-p*gini(left)-(1-p)*gini(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the best split can be defined using the functions, we have created so far. The duty of this function is to find the **best gain** and the **best question** to ask. \n",
    "\n",
    "First, we take the values of a column and send them one by one to the *Question* class, to select a question. Then the data and selected question put into the *partition* function. This process continues until both *true_rows* and *false_rows* lists get elements. After that, *gain* is calculated using *info_gain* function. If this gain value is the best value then the gain and question return as the best choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(rows):\n",
    "    # find the best question to ask\n",
    "    best_gain=0\n",
    "    best_question= None\n",
    "    current_uncertainty=gini(rows)\n",
    "    n_features=len(rows[0])-1\n",
    "    \n",
    "    for col in range(n_features):\n",
    "        values=set([row[col] for row in rows])\n",
    "        \n",
    "        for val in values:\n",
    "            question=Question(col,val)\n",
    "            \n",
    "            true_rows,false_rows=partition(rows,question)\n",
    "            \n",
    "            if len(true_rows)==0 or len(false_rows)==0:\n",
    "                continue\n",
    "                \n",
    "            gain=info_gain(true_rows,false_rows,current_uncertainty)\n",
    "            \n",
    "            if gain>=best_gain:\n",
    "                best_gain,best_question=gain,question\n",
    "    return best_gain,best_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
